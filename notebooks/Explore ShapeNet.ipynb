{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShapeNet\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ShapeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category = 'Airplane'  # Pass in `None` to train on all categories.\n",
    "path = osp.realpath(osp.join('..', 'data', 'ShapeNet'))\n",
    "\n",
    "pre_transform, transform = T.NormalizeScale(), T.FixedPoints(128)\n",
    "train_dataset = ShapeNet(path, category, split='train', transform=transform, pre_transform=pre_transform)\n",
    "valid_dataset = ShapeNet(path, category, split='val', transform=transform, pre_transform=pre_transform)\n",
    "test_dataset = ShapeNet(path, category, split='test', transform=transform, pre_transform=pre_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=6)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpcs.utils.viz import plot_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.x contains points features extracted after having applied the transform\n",
    "# data.pos contains the points coordinates\n",
    "plotter = plot_cloud(data.x.numpy(), scalars=data.y.numpy(), point_size=3.0, notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model' from 'train_3Dhyperbolic' (C:\\Users\\olvcp\\PycharmProjects\\HPCS\\train_3Dhyperbolic.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrain_3Dhyperbolic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m model\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'model' from 'train_3Dhyperbolic' (C:\\Users\\olvcp\\PycharmProjects\\HPCS\\train_3Dhyperbolic.py)"
     ]
    }
   ],
   "source": [
    "from train_3Dhyperbolic import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'load_from_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m best_model \u001B[38;5;241m=\u001B[39m wandb\u001B[38;5;241m.\u001B[39mrestore(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, run_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpierreoo/HPCS/runs/pmcouv1r\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mbest_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_checkpoint\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, run_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpierreoo/HPCS/runs/pmcouv1r\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_io.TextIOWrapper' object has no attribute 'load_from_checkpoint'"
     ]
    }
   ],
   "source": [
    "best_model = wandb.restore('model.h5', run_path='pierreoo/HPCS/runs/pmcouv1r')\n",
    "best_model.load_from_checkpoint('model.h5', run_path='pierreoo/HPCS/runs/pmcouv1r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olvcp\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:\\\\Users\\\\olvcp\\\\PycharmProjects\\\\HPCS\\\\notebooks\\\\model.h5' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unwrapping the module did not yield a `LightningModule`, got <class '_io.TextIOWrapper'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[1;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    722\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 723\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:976\u001B[0m, in \u001B[0;36mTrainer._test_impl\u001B[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001B[0m\n\u001B[0;32m    975\u001B[0m \u001B[38;5;66;03m# links data to the trainer\u001B[39;00m\n\u001B[1;32m--> 976\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattach_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatamodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[0;32m    979\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39mmodel_provided, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    980\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:160\u001B[0m, in \u001B[0;36mDataConnector.attach_data\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, test_dataloaders, predict_dataloaders, datamodule)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;66;03m# set local properties on the model\u001B[39;00m\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_copy_trainer_model_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:163\u001B[0m, in \u001B[0;36mDataConnector._copy_trainer_model_properties\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_copy_trainer_model_properties\u001B[39m(\u001B[38;5;28mself\u001B[39m, model):\n\u001B[1;32m--> 163\u001B[0m     ref_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m model\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m [model, ref_model]:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:2151\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2148\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m   2149\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   2150\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[1;32m-> 2151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:312\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 312\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\overrides\\base.py:116\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[1;34m(wrapped_model)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[1;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class '_io.TextIOWrapper'> instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:938\u001B[0m, in \u001B[0;36mTrainer.test\u001B[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;124;03mPerform one evaluation epoch over the test set.\u001B[39;00m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;124;03mIt's separated from fit to make sure you never run on your test set until you want to.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    935\u001B[0m \u001B[38;5;124;03m    The length of the list corresponds to the number of test dataloaders used.\u001B[39;00m\n\u001B[0;32m    936\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    937\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[1;32m--> 938\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_test_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:737\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[1;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m distributed_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    735\u001B[0m     \u001B[38;5;66;03m# try syncing remaining processes, kill otherwise\u001B[39;00m\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mreconciliate_processes(traceback\u001B[38;5;241m.\u001B[39mformat_exc())\n\u001B[1;32m--> 737\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_exception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexception\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n\u001B[0;32m    739\u001B[0m \u001B[38;5;66;03m# teardown might access the stage so we reset it after\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1619\u001B[0m, in \u001B[0;36mTrainer._call_callback_hooks\u001B[1;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1616\u001B[0m             fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m-> 1619\u001B[0m pl_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n\u001B[0;32m   1620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[0;32m   1621\u001B[0m     prev_fx_name \u001B[38;5;241m=\u001B[39m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:2151\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2148\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m   2149\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   2150\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[1;32m-> 2151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:312\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hyperbolic\\lib\\site-packages\\pytorch_lightning\\overrides\\base.py:116\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[1;34m(wrapped_model)\u001B[0m\n\u001B[0;32m    114\u001B[0m     model \u001B[38;5;241m=\u001B[39m unwrap_lightning_module(model\u001B[38;5;241m.\u001B[39mmodule)\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[1;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class '_io.TextIOWrapper'> instead."
     ]
    }
   ],
   "source": [
    "results = trainer.test(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpcs.utils.viz import plot_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pytorch Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.nn(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch.x, batch.y\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpl_mod = SimpleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(name='SegTest',save_dir='../logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=10, max_epochs=10,logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=simpl_mod, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (HPCS)",
   "language": "python",
   "name": "pycharm-c1fd9184"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}